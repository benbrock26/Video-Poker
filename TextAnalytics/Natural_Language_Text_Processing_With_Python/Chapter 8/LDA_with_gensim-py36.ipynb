{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA in Gensim\n",
    "https://radimrehurek.com/gensim/models/ldamodel.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 8.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# want to make clean words and return a list of tokens\n",
    "\n",
    "#from spacy.en import English\n",
    "#parser = English()\n",
    "\n",
    "import spacy\n",
    "parser = spacy.load('en')\n",
    "\n",
    "def tokenize(text):\n",
    "    lda_tokens = []\n",
    "    tokens = parser(text)\n",
    "    for token in tokens:\n",
    "        if token.orth_.isspace():\n",
    "            continue\n",
    "        elif token.like_url:\n",
    "            lda_tokens.append('URL')\n",
    "        elif token.orth_.startswith('@'):\n",
    "            lda_tokens.append('SCREEN_NAME')\n",
    "        else:\n",
    "            lda_tokens.append(token.lower_)\n",
    "    return lda_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['SCREEN_NAME', 'said', 'the', '#', 'chicken', 'was', 'at', 'the', '#', 'junkyard', '.', 'see', 'URL', '.']\n",
      "Wall time: 110 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "sent = '@bob said the #chicken was at the #junkyard. See http://www.jonathanmugan.com.'\n",
    "out_tokens = tokenize(sent)\n",
    "print(out_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 4.21 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# we want to lemmatize so dogs goes to dog and ran goes to run\n",
    "# Lemmatization means to get the \"dictionary entry\" for a word\n",
    "\n",
    "# Some documentation here http://www.nltk.org/howto/wordnet.html\n",
    "\n",
    "from nltk.corpus import wordnet as wn\n",
    "def get_lemma(word):\n",
    "    lemma = wn.morphy(word)\n",
    "    if lemma is None:\n",
    "        return word\n",
    "    else: \n",
    "        return lemma\n",
    "    \n",
    "# or can use this\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "def get_lemma2(word):\n",
    "    return WordNetLemmatizer().lemmatize(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dogs dog dog\n",
      "ran run ran\n",
      "discouraged discourage discouraged\n",
      "Wall time: 2.51 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for w in ['dogs','ran','discouraged']:\n",
    "    print(w,get_lemma(w),get_lemma2(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 258 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import nltk\n",
    "en_stop = set(nltk.corpus.stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def prepare_text_for_lda(text):\n",
    "    tokens = tokenize(text)\n",
    "    tokens = [token for token in tokens if len(token) > 4]\n",
    "    tokens = [token for token in tokens if token not in en_stop]\n",
    "    tokens = [get_lemma(token) for token in tokens]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['enjoy', 'going', 'restaurant', 'hamburger']\n",
      "Wall time: 19.5 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "sent = 'I enjoy going to restaurants to eat hamburgers.'\n",
    "print(prepare_text_for_lda(sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['yesterday', 'nothing', 'significant', 'product', 'epiphany', 'moment', 'today']\n",
      "['finish', 'breakfast', 'champion', 'vonnegut', 'yesterday', 'book', 'love']\n",
      "['first', 'breakfast', 'buy', 'another', 'airportfood']\n",
      "['looking', 'refrigerator', 'harvest', 'world']\n",
      "['tomato', 'really', 'settle', 'stomach', 'hope', 'would']\n",
      "['think', 'another', 'century', 'actually', 'guess']\n",
      "['amaze', 'story', 'make', 'experience', 'movie', 'times', 'place', 'association']\n",
      "['drove', 'water', 'softener', 'front', 'hassle', 'carsarestupid']\n",
      "['people', 'realize', 'dangerous', 'could', 'confuse', 'saving', 'cousin']\n",
      "['cognitive', 'humility', 'crucial', 'adapt', 'world', 'constantly', 'change', 'curiosity', 'cycle']\n",
      "['imagine', 'freeze', 'focus', 'reality', 'relationship', 'princess', 'hermit', 'doesnotendwell']\n",
      "['want', 'movie', 'relax', 'could', 'worse', 'choice', 'different', 'perhaps']\n",
      "['place', 'klout', 'wrong', 'perk', 'perk', 'potential', 'perk', 'advertisement']\n",
      "['record', 'history', 'small', 'history', 'humanity', 'amaze', 'think', 'story']\n",
      "['qualia', 'feel', 'means', 'similar', 'relative']\n",
      "['science', 'outlier', 'either', 'throw', 'weird', 'closely', 'examine', 'interest']\n",
      "['meditate', 'recognize', 'become']\n",
      "['story', 'contain', 'contain', 'story']\n",
      "['fourth', 'grade', 'teacher', 'polishing', 'speech', 'third', 'grade', 'anymore', 'SCREEN_NAME']\n",
      "['regrets\"\\x9d', 'likely', 'marshmallow', 'instead', 'waiting', 'regret', 'learn']\n",
      "['front', 'midnight', 'stroll', 'snake', 'sitting', 'looking', 'guess', 'watch', 'movie', 'instead']\n",
      "['daughter', 'ask', 'start', 'successor', 'function', 's(s(0', 's(s(s(s(s(0']\n",
      "['determinist', 'insurance', 'company', 'insist', 'conditions', 'preexist', 'headline', 'onion']\n",
      "['better', 'estate', 'agent', 'might', 'compute', 'commission', 'subtract', 'baseline', 'price']\n",
      "['group', 'chimpanzee', 'murder', 'solitary', 'male', 'tribe', 'wonder', 'afraid', 'downstairs']\n",
      "['twitter', 'nothing', 'tweet', 'make', 'dully']\n",
      "['depress', 'primitive', 'brain', 'science', 'playing', 'radio', 'still', 'remove', 'battery']\n",
      "['watch', 'rover', 'take', 'theft', 'seriously', 'actually', 'often', 'vague', 'movie', 'understand']\n",
      "['daughter', 'fill', 'phone', 'selfies', 'soitbegins']\n",
      "['little', 'dopamine', 'connection', 'writer', 'drink', 'coming']\n",
      "['funny', 'think', 'digitally', 'picture', 'picture']\n",
      "['alzheimer', 'belief', 'people', 'watch', 'nebraska', 'recently']\n",
      "['rogue', 'drone', 'rapidly', 'become', 'national', 'nuisance', 'mention', 'annoying', 'sound']\n",
      "['reading', 'oliver', 'twist', 'light', 'sarcastic', 'story', 'make', 'horror', 'deep', 'sneak']\n",
      "['listen', 'first', 'episode', 'podcast', 'serial', 'pretty', 'investment', 'withhold', 'fact']\n",
      "['finish', 'reading', 'night', 'wiesel', 'first', 'account', 'holocaust', 'asking', 'could', 'happen', 'modern', 'times']\n",
      "['pretty', 'funny', 'google', 'slide', 'handle', 'powerpoint', 'equation', 'powerpoint']\n",
      "['first', 'building', 'machine', 'fundamental', 'commonsense', 'knowledge', 'identify', 'profitable', 'task', 'require', 'knowledge']\n",
      "['lunch', 'probably', 'accurately', 'describe', 'lunch', 'luggage']\n",
      "['still', 'remember', 'metallic', 'smell', 'incredible', 'lunch', 'first', 'grade']\n",
      "['school', 'learn', 'concept', 'a-->b-->c.', 'boring', 'instead', 'could']\n",
      "['baffled', 'profound', 'concept', 'learn', 'assume', 'eventually', 'figure']\n",
      "['dyson', 'argue', 'undergo', 'digital', 'claim', 'metaphor', 'birth', 'equally', 'important', 'universe']\n",
      "['learning', 'cooking', 'ingredient', 'together', 'periodically', 'check', 'modelalmostready']\n",
      "['headline', 'meditation', 'devices', 'mindful', 'entrepreneur', 'present', 'presumably', 'seriousness']\n",
      "['wonder', 'advance', 'alien', 'race', 'version', 'incompatibility', 'software', 'change', 'function', 'signature']\n",
      "['downstairs', 'coffee', 'expect', 'someone', 'family', 'coffee', 'closer']\n",
      "['participate', 'pentathlon', 'board', 'game', 'challenge', 'move', 'opponent', 'lawyer', 'training']\n",
      "['dislike', 'restaurant', 'ignore', 'something', 'explicitly', 'design', 'capture', 'attention']\n",
      "['finally', 'watch', 'latest', 'realize', 'movie']\n",
      "['sugar', 'enemy', 'sugar', 'give', 'productivity']\n",
      "['buzzfeed', 'middle', 'people', 'celebrity']\n",
      "['password', 'enter', 'incorrect', 'maybe', 'service', 'server', 'sync', 'anything', 'really']\n",
      "['garage']\n",
      "['funny', 'sport', 'thing']\n",
      "['try', 'teach', 'perspective', 'noise', 'great', 'thing']\n",
      "['lobbyist', 'exactly', 'would', 'anyone', 'power', 'listen', 'someone', 'opinion']\n",
      "['guess', 'years', 'military', 'superiority', 'nation', 'powerful', 'artificial', 'intelligence']\n",
      "['burn', 'finger', 'boiling', 'vegetable', 'healthful', 'claim']\n",
      "['funny', 'little', 'change', 'years', 'school', 'middle', 'night', 'years', 'later', 'still']\n",
      "['funny', 'immediately', 'become', 'thirsty', 'moment', 'water']\n",
      "['going', 'bookstore', 'fashion', 'version', 'surfing']\n",
      "['ask', 'build', 'great', 'china', 'mongolian', 'learn', 'southpark']\n",
      "['finally', 'watch', 'glengarry', 'coffee', 'closer']\n",
      "['would', 'think', 'would', 'finite', 'number', 'could', 'would', 'wrong']\n",
      "['memory', 'sneak', 'gloria', 'estefan', '1980s']\n",
      "['surprise', 'recommendation', 'engine', 'goodreads', 'book', 'indication', 'might']\n",
      "['write', 'would', 'forget', 'remember', 'piece', 'paper']\n",
      "['soccer', 'practice', 'would', 'money', 'entire', 'score', 'goalllll']\n",
      "['receive', 'today', 'company', 'offering', 'black', 'facilitation', 'government', 'contract', 'blackmail', 'decision', 'maker']\n",
      "['point', 'pass', 'threshold', 'picture', 'internet', 'actual', 'world']\n",
      "['today', 'attach', 'blaring', 'little', 'closer', 'idiocracy', 'every']\n",
      "['someday', 'future', 'archaeologist', 'uncover', 'countless', 'credit', 'offer', 'every', 'piece']\n",
      "['amaze', 'still', 'hostage', 'messaging', 'either', 'imessage', 'iphone', 'monthly', 'utterance']\n",
      "['try', 'frame', 'experience', 'lift', 'weight', 'framing', 'working', 'meditation', 'work', 'night']\n",
      "['sympathize', 'view', 'trouble', 'trust', 'michael', 'moore', 'since', 'blame', 'clark', 'death', 'little']\n",
      "['probably', 'social', 'medium', 'morning', 'things', 'would', 'sense']\n",
      "['service', 'allow', 'twitter', 'account', 'threaten', 'friend']\n",
      "['generally', 'prefer', 'quiet', 'sound', 'crumble', 'infrastructure', 'noise', 'construction', 'viable', 'approach']\n",
      "['coffee', 'place', 'taste', 'fancy', 'fancy']\n",
      "['minutes', 'bedtime', 'night', 'decide', 'install', 'ubuntu', 'smart', 'insomnia', 'feelterribletoday']\n",
      "['future', 'waste', 'fixing', 'toilet']\n",
      "['squishy', 'current', 'small', 'useful', 'idea', 'form', 'practical']\n",
      "['digit', 'country', 'suite', 'reward', 'number', 'prepare', 'intergalactic', 'expansion', 'thinkbig']\n",
      "['dream', 'bronze', 'medal', 'swimming', 'dream', 'walk', 'barefoot', 'looking', 'water', 'first', 'dream', 'prefer']\n",
      "['getting', 'vocabulary', 'straight', 'getting', 'thinking', 'straight', 'correctness', 'important', 'consistency']\n",
      "['world', 'documentation']\n",
      "['drink', 'coffee']\n",
      "['twenty', 'years', 'meaning', '1970s', 'twenty', 'years']\n",
      "['aphorism', 'middle', 'phone', 'rings', 'saturday', 'night']\n",
      "['hotel', 'ramada', 'waving', 'front', 'going', 'together', 'attack', 'holiday', 'across', 'street']\n",
      "['restaurant', 'taste', 'better', 'compare', 'item', 'compare', 'sandwich']\n",
      "['downstairs', 'class', 'water', 'eating', 'piece', 'judge', 'short', 'enjoyment', 'verse']\n",
      "['start', 'using', 'phrase', 'evidently', 'second', 'grade', 'curriculum', 'change', 'since']\n",
      "['online', 'application', 'online', 'available', 'certain', 'hours', 'unclearontheconcept']\n",
      "['watch', 'mulholland', 'drive', 'comprendo']\n",
      "['surprisingly', 'search', 'feature', 'google', 'access', 'search', 'expert']\n",
      "['initial', 'euphoria', 'great', 'funny', 'fade', 'reality']\n",
      "['company', 'outsource', 'money', 'customer', 'service', 'better', 'solution', 'would', 'ambiguous', 'error', 'ride', 'letters', 'customer']\n",
      "['cleaning', 'saying', 'another', 'throw', 'blood', 'zombie']\n",
      "['cloud', 'lose', 'google', 'phone', 'number']\n",
      "['stratego', 'great', 'teaching', 'value', 'information', 'reveal', 'information', 'opponent']\n",
      "['always', 'confuse']\n",
      "['start', 'bagel', 'going', 'otherwise', 'crash']\n",
      "['younger', 'soreness', 'muscle', 'playing', 'soccer', 'mean', 'work', 'soreness', 'hurt']\n",
      "['outside', 'pruning', 'tree', 'master']\n",
      "['buying', 'pyrite', 'something', 'wrong']\n",
      "['spelling', 'spell', 'check', 'intend', 'suggestion']\n",
      "['tablet', 'powerpoint', 'create', 'slide', 'drawing', 'finger', 'writing']\n",
      "['itunes', 'music', 'place', 'brain', 'first', 'itunes', 'music', 'belong']\n",
      "['wonder', 'would', 'raise', 'radically', 'different', 'circumstances', 'question', 'sense', 'would']\n",
      "['wrestling', 'little', 'tomorrow']\n",
      "['however', 'branch']\n",
      "['idea', 'funny', 'thing', 'fragment', 'obvious', 'exist']\n",
      "['cartoon', 'husband', 'living', 'looking', 'internet']\n",
      "['anyone', 'notice', 'automatic', 'language', 'translation', 'finally', 'seem', 'crazy']\n",
      "['need', 'statistics', 'science', 'program', 'parameter', 'report']\n",
      "['dream', 'learning', 'simulate', 'experience', 'dream', 'place', 'dream', 'alter', 'familiar', 'place']\n",
      "Wall time: 27 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# get the data\n",
    "import random\n",
    "text_data = []\n",
    "with open('jonathan_mugan_tweets.txt', encoding=\"utf8\") as f:\n",
    "    for line in f:\n",
    "        tokens = prepare_text_for_lda(line)\n",
    "        if random.random() > .95:\n",
    "            print(tokens)\n",
    "        text_data.append(tokens)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ben Brock\\Anaconda3\\envs\\py36_tweet_spacy\\lib\\site-packages\\gensim-3.3.0-py3.6-win-amd64.egg\\gensim\\utils.py:1167: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 6.42 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# create a dictionary from the data\n",
    "from gensim import corpora\n",
    "dictionary = corpora.Dictionary(text_data)\n",
    "\n",
    "# Warning message shows that you can also do lemmatization through Pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 31 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# convert to bag-of-words corpus\n",
    "corpus = [dictionary.doc2bow(text) for text in text_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 14 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# save the corpus and dictionary, we will use these in another video to visualize\n",
    "import pickle\n",
    "pickle.dump( corpus, open( \"corpus.pkl\", \"wb\" ) )\n",
    "dictionary.save('dictionary.gensim')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 23.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import gensim\n",
    "NUM_TOPICS = 5\n",
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=NUM_TOPICS,\n",
    "                                           id2word= dictionary, passes = 15)\n",
    "ldamodel.save('model5.gensim')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, '0.014*\"want\" + 0.011*\"remember\" + 0.011*\"memory\" + 0.009*\"going\"')\n",
      "(1, '0.016*\"coffee\" + 0.009*\"funny\" + 0.007*\"people\" + 0.006*\"would\"')\n",
      "(2, '0.012*\"people\" + 0.010*\"amaze\" + 0.009*\"could\" + 0.008*\"think\"')\n",
      "(3, '0.010*\"child\" + 0.010*\"people\" + 0.008*\"story\" + 0.008*\"funny\"')\n",
      "(4, '0.013*\"watch\" + 0.011*\"would\" + 0.008*\"great\" + 0.008*\"funny\"')\n",
      "Wall time: 14.5 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "topics = ldamodel.print_topics(num_words=4)\n",
    "for topic in topics:\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(8, 1), (191, 1)]\n",
      "[(0, 0.067139052), (1, 0.067170478), (2, 0.068138503), (3, 0.068003409), (4, 0.72954857)]\n",
      "Wall time: 19.5 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# try a new document\n",
    "# we see it is mostly topic 3\n",
    "new_doc = 'I watch movies.'\n",
    "new_doc = prepare_text_for_lda(new_doc)\n",
    "new_doc_bow = dictionary.doc2bow(new_doc)\n",
    "print(new_doc_bow)\n",
    "print(ldamodel.get_document_topics(new_doc_bow))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, '0.012*\"funny\" + 0.011*\"people\" + 0.007*\"dream\" + 0.007*\"computer\"')\n",
      "(1, '0.013*\"would\" + 0.008*\"always\" + 0.007*\"think\" + 0.006*\"something\"')\n",
      "(2, '0.010*\"could\" + 0.010*\"want\" + 0.007*\"watch\" + 0.006*\"remember\"')\n",
      "Wall time: 27.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# try three topics\n",
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=3,\n",
    "                                           id2word= dictionary, passes = 15)\n",
    "ldamodel.save('model3.gensim')\n",
    "topics = ldamodel.print_topics(num_words=4)\n",
    "for topic in topics:\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, '0.012*\"interest\" + 0.012*\"drink\" + 0.012*\"video\" + 0.011*\"people\"')\n",
      "(1, '0.019*\"movie\" + 0.018*\"watch\" + 0.017*\"going\" + 0.012*\"could\"')\n",
      "(2, '0.015*\"amaze\" + 0.012*\"people\" + 0.010*\"could\" + 0.010*\"still\"')\n",
      "(3, '0.013*\"night\" + 0.012*\"around\" + 0.010*\"someone\" + 0.010*\"funny\"')\n",
      "(4, '0.022*\"something\" + 0.016*\"funny\" + 0.011*\"would\" + 0.010*\"wrong\"')\n",
      "(5, '0.027*\"dream\" + 0.024*\"remember\" + 0.015*\"memory\" + 0.014*\"want\"')\n",
      "(6, '0.018*\"people\" + 0.013*\"think\" + 0.011*\"anyone\" + 0.010*\"picture\"')\n",
      "(7, '0.018*\"coffee\" + 0.011*\"young\" + 0.009*\"reading\" + 0.009*\"china\"')\n",
      "(8, '0.015*\"always\" + 0.013*\"make\" + 0.012*\"anything\" + 0.010*\"actually\"')\n",
      "(9, '0.025*\"would\" + 0.017*\"computer\" + 0.014*\"people\" + 0.012*\"ask\"')\n",
      "Wall time: 21.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# try ten topics\n",
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=10,\n",
    "                                           id2word= dictionary, passes = 15)\n",
    "ldamodel.save('model10.gensim')\n",
    "topics = ldamodel.print_topics(num_words=4)\n",
    "for topic in topics:\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['DESCR', 'data', 'description', 'filenames', 'target', 'target_names']\n",
      "11314\n",
      "[7 4 4 ..., 3 1 8]\n",
      "['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n",
      "From: lerxst@wam.umd.edu (where's my thing)\n",
      "Subject: WHAT car is this!?\n",
      "Nntp-Posting-Host: rac3.wam.umd.edu\n",
      "Organization: University of Maryland, College Park\n",
      "Lines: 15\n",
      "\n",
      " I was wondering if anyone out there could enlighten me on this car I saw\n",
      "the other day. It was a 2-door sports car, looked to be from the late 60s/\n",
      "early 70s. It was called a Bricklin. The doors were really small. In addition,\n",
      "the front bumper was separate from the rest of the body. This is \n",
      "all I know. If anyone can tellme a model name, engine specs, years\n",
      "of production, where this car is made, history, or whatever info you\n",
      "have on this funky looking car, please e-mail.\n",
      "\n",
      "Thanks,\n",
      "- IL\n",
      "   ---- brought to you by your neighborhood Lerxst ----\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Wall time: 305 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Exercise: Run LDA on Newsgroup Data\n",
    "# The Newsgroup Data\n",
    "# http://scikit-learn.org/stable/datasets/twenty_newsgroups.html#newsgroups\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "texts = fetch_20newsgroups(subset='train')\n",
    "print(dir(texts))\n",
    "# 11,314 posts\n",
    "print(len(texts.target))\n",
    "print(texts.target)\n",
    "print(texts.target_names)\n",
    "print(texts.data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
