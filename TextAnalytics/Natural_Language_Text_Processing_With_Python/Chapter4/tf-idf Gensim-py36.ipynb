{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gensim is a software library located at\n",
    "https://radimrehurek.com/gensim/\n",
    "\n",
    "## How to install Gensim\n",
    "https://radimrehurek.com/gensim/install.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create some raw documents\n",
    "raw_documents = ['I love tacos.',\n",
    "             'She ran with the chicken.',\n",
    "             'I don’t choose to take a nap. The nap chooses me.',\n",
    "            'That man is nice as pie with ice cream.',\n",
    "            'This pizza is an affront to nature.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We are going to need to tokenize, so let's use NLTK\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define a function that makes tokens\n",
    "def get_tokens(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['I', 'love', 'tacos', '.'], ['She', 'ran', 'with', 'the', 'chicken', '.'], ['I', 'don', '’', 't', 'choose', 'to', 'take', 'a', 'nap', '.', 'The', 'nap', 'chooses', 'me', '.'], ['That', 'man', 'is', 'nice', 'as', 'pie', 'with', 'ice', 'cream', '.'], ['This', 'pizza', 'is', 'an', 'affront', 'to', 'nature', '.']]\n"
     ]
    }
   ],
   "source": [
    "# A Gensim document is a list of tokens\n",
    "# We could optionally make all of the tokens lower case\n",
    "gen_docs = [get_tokens(text) for text in raw_documents]\n",
    "print(gen_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num words in dictionary: 33\n",
      "0 .\n",
      "1 I\n",
      "2 love\n",
      "3 tacos\n",
      "4 She\n",
      "5 chicken\n",
      "6 ran\n",
      "7 the\n",
      "8 with\n",
      "9 The\n",
      "10 a\n",
      "11 choose\n",
      "12 chooses\n",
      "13 don\n",
      "14 me\n",
      "15 nap\n",
      "16 t\n",
      "17 take\n",
      "18 to\n",
      "19 ’\n",
      "20 That\n",
      "21 as\n",
      "22 cream\n",
      "23 ice\n",
      "24 is\n",
      "25 man\n",
      "26 nice\n",
      "27 pie\n",
      "28 This\n",
      "29 affront\n",
      "30 an\n",
      "31 nature\n",
      "32 pizza\n"
     ]
    }
   ],
   "source": [
    "# Create dictionary from a list of documents\n",
    "# A dictionary maps every word to a number\n",
    "dictionary = gensim.corpora.Dictionary(gen_docs)\n",
    "num_words = len(dictionary)\n",
    "print(\"Num words in dictionary: {}\".format(num_words))\n",
    "for idx,word in dictionary.items():\n",
    "    print(idx,word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the\n",
      "the\n"
     ]
    }
   ],
   "source": [
    "# Convert token id to string; there are two ways to do it\n",
    "print(dictionary[7])\n",
    "print(dictionary.id2token[7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "# Convert string to token id\n",
    "print(dictionary.token2id['ran'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 1), (2, 3), (3, 1)]\n"
     ]
    }
   ],
   "source": [
    "# Create bag of words\n",
    "# A bag of words is tf term frequency (tf) of tf-idf\n",
    "# Called a \"bag of words\" because order is lost\n",
    "# Note that \"!\" is not listed because it is not in the dictionary\n",
    "bow_doc = dictionary.doc2bow(['I','love','love','love','tacos','!'])\n",
    "print(bow_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(0, 1), (1, 1), (2, 1), (3, 1)], [(0, 1), (4, 1), (5, 1), (6, 1), (7, 1), (8, 1)], [(0, 2), (1, 1), (9, 1), (10, 1), (11, 1), (12, 1), (13, 1), (14, 1), (15, 2), (16, 1), (17, 1), (18, 1), (19, 1)], [(0, 1), (8, 1), (20, 1), (21, 1), (22, 1), (23, 1), (24, 1), (25, 1), (26, 1), (27, 1)], [(0, 1), (18, 1), (24, 1), (28, 1), (29, 1), (30, 1), (31, 1), (32, 1)]]\n"
     ]
    }
   ],
   "source": [
    "# Create corpus\n",
    "# A corpus is a list of bags of words\n",
    "corpus = [dictionary.doc2bow(gen_doc) for gen_doc in gen_docs]\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TfidfModel(num_docs=5, num_nnz=41)\n"
     ]
    }
   ],
   "source": [
    "# Create tf-idf model from corpus\n",
    "# num_nnz is the number of tokens\n",
    "tf_idf = gensim.models.TfidfModel(corpus)\n",
    "print(tf_idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'love', 'tacos', '.']\n",
      "[(0, 1), (1, 1), (2, 1), (3, 1)]\n",
      "[(1, 0.37344696513776354), (2, 0.65594868862945144), (3, 0.65594868862945144)]\n"
     ]
    }
   ],
   "source": [
    "# Show document in text form, bag of words, and tf-idf\n",
    "# 0 is tacos, 1 is love, 2 is I\n",
    "# Value for I is lower because occurs multiple times.\n",
    "# Value for '.' is 0 because it occurs in all sentences and log_2(1) = 0.\n",
    "# Vectors are normalized so they sum to 1\n",
    "print(gen_docs[0])\n",
    "print(corpus[0])\n",
    "print(tf_idf[corpus][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1), (1, 1), (2, 1), (32, 1)]\n",
      "[(1, 0.37344696513776354), (2, 0.65594868862945144), (32, 0.65594868862945144)]\n"
     ]
    }
   ],
   "source": [
    "# Show bag of words and tf-idf for new document\n",
    "# Note it is similar to to document above\n",
    "bow = dictionary.doc2bow(['I','love','pizza','.'])\n",
    "print(bow)\n",
    "print(tf_idf[bow])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2.32192809  2.32192809  1.32192809]\n",
      "3.53980141303\n",
      "[ 0.65594869  0.65594869  0.37344697]\n"
     ]
    }
   ],
   "source": [
    "# This is just a confirmation. Create tf-idf vector manually. Left as an exercise.\n",
    "# idf if it occurs once in corpus (like \"tacos\" and \"love\")\n",
    "# idf if it occurs twice in corpus (like \"I\")\n",
    "from math import log\n",
    "num_docs = tf_idf.num_docs\n",
    "idf_1 = log(num_docs/1,2)\n",
    "idf_2 = log(num_docs/2,2)\n",
    "# only show nonzero values, and use numpy array\n",
    "import numpy as np\n",
    "v = np.array([idf_1,idf_1,idf_2])\n",
    "print(v)\n",
    "# normalize to the length is 1\n",
    "norm_v = np.linalg.norm(v)\n",
    "print(norm_v)\n",
    "# Show normalized vector\n",
    "print(v/norm_v)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity index with 5 documents in 0 shards (stored under /Users/jmugan/)\n"
     ]
    }
   ],
   "source": [
    "# Create similarity measure object in tf-idf space\n",
    "# First arg is temp external storage\n",
    "# https://radimrehurek.com/gensim/similarities/docsim.html\n",
    "sims = gensim.similarities.Similarity('/Users/jmugan/',tf_idf[corpus],\n",
    "                                      num_features=len(dictionary))\n",
    "print(sims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity index with 5 documents in 0 shards (stored under C:\\Users\\Ben Brock\\Documents\\GitHub\\Bens_Portfolio\\Data_Mining\\DataScience-Projects\\LDA\\Natural_Language_Text_Processing_With_Python\\Chapter 4\\tst_py36)\n"
     ]
    }
   ],
   "source": [
    "# Create similarity measure object in tf-idf space\n",
    "# First arg is temp external storage\n",
    "# https://radimrehurek.com/gensim/similarities/docsim.html\n",
    "# - C:\\Users\\Ben Brock\\Documents\\GitHub\\Bens_Portfolio\\Data_Mining\\DataScience-Projects\\LDA\\Natural_Language_Text_Processing_With_Python\n",
    "sims = gensim.similarities.Similarity('C:\\\\Users\\\\Ben Brock\\\\Documents\\\\GitHub\\\\Bens_Portfolio\\\\Data_Mining\\\\DataScience-Projects\\\\LDA\\\\Natural_Language_Text_Processing_With_Python\\\\Chapter 4\\\\tst_py36',\n",
    "                                      tf_idf[corpus],\n",
    "                                      num_features=len(dictionary))\n",
    "print(sims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['chicken', 'with', 'tacos', 'love']\n",
      "[(2, 1), (3, 1), (5, 1), (8, 1)]\n",
      "[(2, 0.54848032538919966), (3, 0.54848032538919966), (5, 0.54848032538919966), (8, 0.31226270667960454)]\n"
     ]
    }
   ],
   "source": [
    "# Create query document and convert to tf-idf\n",
    "# doc shares two words with each of first two docs in corpus\n",
    "query_doc = \"chicken with tacos love\".split()\n",
    "print(query_doc)\n",
    "query_doc_bow = dictionary.doc2bow(query_doc)\n",
    "print(query_doc_bow)\n",
    "query_doc_tf_idf = tf_idf[query_doc_bow]\n",
    "print(query_doc_tf_idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.71954989,  0.34925455,  0.        ,  0.06428327,  0.        ], dtype=float32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show array of document similarities to query\n",
    "# Also both document 0 and 1 match with two words each,\n",
    "# document 1 matches with word \"with\" that occurs twice in corpus.\n",
    "# Only one overlapping word with the fourth document\n",
    "sims[query_doc_tf_idf]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Exercise: take a document like an email or news article and find sentences most similar to input query sentences."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
