{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gensim is a software library located at\n",
    "https://radimrehurek.com/gensim/\n",
    "\n",
    "## How to install Gensim\n",
    "https://radimrehurek.com/gensim/install.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ben Brock\\Anaconda3\\envs\\py27_gensim_spacy\\lib\\site-packages\\gensim\\utils.py:860: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create some raw documents\n",
    "raw_documents = ['I love tacos.',\n",
    "             'She ran with the chicken.',\n",
    "             'I donâ€™t choose to take a nap. The nap chooses me.',\n",
    "            'That man is nice as pie with ice cream.',\n",
    "            'This pizza is an affront to nature.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We are going to need to tokenize, so let's use NLTK\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define a function that makes tokens\n",
    "def get_tokens(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['I', 'love', 'tacos', '.'], ['She', 'ran', 'with', 'the', 'chicken', '.'], ['I', 'don\\xe2\\x80\\x99t', 'choose', 'to', 'take', 'a', 'nap', '.', 'The', 'nap', 'chooses', 'me', '.'], ['That', 'man', 'is', 'nice', 'as', 'pie', 'with', 'ice', 'cream', '.'], ['This', 'pizza', 'is', 'an', 'affront', 'to', 'nature', '.']]\n"
     ]
    }
   ],
   "source": [
    "# A Gensim document is a list of tokens\n",
    "# We could optionally make all of the tokens lower case\n",
    "gen_docs = [get_tokens(text) for text in raw_documents]\n",
    "print(gen_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num words in dictionary: 31\n",
      "(2, u'love')\n",
      "(4, u'ran')\n",
      "(19, u'is')\n",
      "(20, u'pie')\n",
      "(28, u'an')\n",
      "(22, u'as')\n",
      "(8, u'chicken')\n",
      "(25, u'cream')\n",
      "(11, u'don\\u2019t')\n",
      "(18, u'That')\n",
      "(12, u'chooses')\n",
      "(21, u'ice')\n",
      "(3, u'.')\n",
      "(13, u'to')\n",
      "(14, u'nap')\n",
      "(15, u'choose')\n",
      "(17, u'take')\n",
      "(10, u'me')\n",
      "(30, u'nature')\n",
      "(0, u'I')\n",
      "(26, u'This')\n",
      "(1, u'tacos')\n",
      "(7, u'She')\n",
      "(16, u'The')\n",
      "(6, u'with')\n",
      "(23, u'man')\n",
      "(9, u'a')\n",
      "(29, u'affront')\n",
      "(27, u'pizza')\n",
      "(5, u'the')\n",
      "(24, u'nice')\n"
     ]
    }
   ],
   "source": [
    "# Create dictionary from a list of documents\n",
    "# A dictionary maps every word to a number\n",
    "dictionary = gensim.corpora.Dictionary(gen_docs)\n",
    "num_words = len(dictionary)\n",
    "print(\"Num words in dictionary: {}\".format(num_words))\n",
    "for idx,word in dictionary.items():\n",
    "    print(idx,word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "with\n",
      "with\n"
     ]
    }
   ],
   "source": [
    "# Convert token id to string; there are two ways to do it\n",
    "print(dictionary[6])\n",
    "print(dictionary.id2token[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "# Convert string to token id\n",
    "print(dictionary.token2id['ran'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1), (1, 1), (2, 3)]\n"
     ]
    }
   ],
   "source": [
    "# Create bag of words\n",
    "# A bag of words is tf term frequency (tf) of tf-idf\n",
    "# Called a \"bag of words\" because order is lost\n",
    "# Note that \"!\" is not listed because it is not in the dictionary\n",
    "bow_doc = dictionary.doc2bow(['I','love','love','love','tacos','!'])\n",
    "print(bow_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(0, 1), (1, 1), (2, 1), (3, 1)], [(3, 1), (4, 1), (5, 1), (6, 1), (7, 1), (8, 1)], [(0, 1), (3, 2), (9, 1), (10, 1), (11, 1), (12, 1), (13, 1), (14, 2), (15, 1), (16, 1), (17, 1)], [(3, 1), (6, 1), (18, 1), (19, 1), (20, 1), (21, 1), (22, 1), (23, 1), (24, 1), (25, 1)], [(3, 1), (13, 1), (19, 1), (26, 1), (27, 1), (28, 1), (29, 1), (30, 1)]]\n"
     ]
    }
   ],
   "source": [
    "# Create corpus\n",
    "# A corpus is a list of bags of words\n",
    "corpus = [dictionary.doc2bow(gen_doc) for gen_doc in gen_docs]\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TfidfModel(num_docs=5, num_nnz=39)\n"
     ]
    }
   ],
   "source": [
    "# Create tf-idf model from corpus\n",
    "# num_nnz is the number of tokens\n",
    "tf_idf = gensim.models.TfidfModel(corpus)\n",
    "print(tf_idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'love', 'tacos', '.']\n",
      "[(0, 1), (1, 1), (2, 1), (3, 1)]\n",
      "[(0, 0.37344696513776354), (1, 0.6559486886294514), (2, 0.6559486886294514)]\n"
     ]
    }
   ],
   "source": [
    "# Show document in text form, bag of words, and tf-idf\n",
    "# 0 is tacos, 1 is love, 2 is I\n",
    "# Value for I is lower because occurs multiple times.\n",
    "# Value for '.' is 0 because it occurs in all sentences and log_2(1) = 0.\n",
    "# Vectors are normalized so they sum to 1\n",
    "print(gen_docs[0])\n",
    "print(corpus[0])\n",
    "print(tf_idf[corpus][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1), (2, 1), (3, 1), (27, 1)]\n",
      "[(0, 0.37344696513776354), (2, 0.6559486886294514), (27, 0.6559486886294514)]\n"
     ]
    }
   ],
   "source": [
    "# Show bag of words and tf-idf for new document\n",
    "# Note it is similar to to document above\n",
    "bow = dictionary.doc2bow(['I','love','pizza','.'])\n",
    "print(bow)\n",
    "print(tf_idf[bow])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2.32192809  2.32192809  1.        ]\n",
      "3.43259379415\n",
      "[ 0.67643544  0.67643544  0.29132489]\n"
     ]
    }
   ],
   "source": [
    "# This is just a confirmation. Create tf-idf vector manually. Left as an exercise.\n",
    "# idf if it occurs once in corpus (like \"tacos\" and \"love\")\n",
    "# idf if it occurs twice in corpus (like \"I\")\n",
    "from math import log\n",
    "num_docs = tf_idf.num_docs\n",
    "idf_1 = log(num_docs/1,2)\n",
    "idf_2 = log(num_docs/2,2)\n",
    "# only show nonzero values, and use numpy array\n",
    "import numpy as np\n",
    "v = np.array([idf_1,idf_1,idf_2])\n",
    "print(v)\n",
    "# normalize to the length is 1\n",
    "norm_v = np.linalg.norm(v)\n",
    "print(norm_v)\n",
    "# Show normalized vector\n",
    "print(v/norm_v)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity index with 5 documents in 0 shards (stored under /Users/jmugan/)\n"
     ]
    }
   ],
   "source": [
    "# Create similarity measure object in tf-idf space\n",
    "# First arg is temp external storage\n",
    "# https://radimrehurek.com/gensim/similarities/docsim.html\n",
    "sims = gensim.similarities.Similarity('/Users/jmugan/',tf_idf[corpus],\n",
    "                                      num_features=len(dictionary))\n",
    "print(sims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ben Brock\\Documents\\GitHub\\Bens_Portfolio\\Data_Mining\\DataScience-Projects\\LDA\\Natural_Language_Text_Processing_With_Python\\Chapter 4\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity index with 5 documents in 0 shards (stored under C:\\Users\\Ben Brock\\Documents\\GitHub\\Bens_Portfolio\\Data_Mining\\DataScience-Projects\\LDA\\Natural_Language_Text_Processing_With_Python\\Chapter 4\\tst)\n"
     ]
    }
   ],
   "source": [
    "# Create similarity measure object in tf-idf space\n",
    "# First arg is temp external storage\n",
    "# https://radimrehurek.com/gensim/similarities/docsim.html\n",
    "# - C:\\Users\\Ben Brock\\Documents\\GitHub\\Bens_Portfolio\\Data_Mining\\DataScience-Projects\\LDA\\Natural_Language_Text_Processing_With_Python\n",
    "sims = gensim.similarities.Similarity('C:\\\\Users\\\\Ben Brock\\\\Documents\\\\GitHub\\\\Bens_Portfolio\\\\Data_Mining\\\\DataScience-Projects\\\\LDA\\\\Natural_Language_Text_Processing_With_Python\\\\Chapter 4\\\\tst',\n",
    "                                      tf_idf[corpus],\n",
    "                                      num_features=len(dictionary))\n",
    "print(sims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['chicken', 'with', 'tacos', 'love']\n",
      "[(1, 1), (2, 1), (6, 1), (8, 1)]\n",
      "[(1, 0.5484803253891997), (2, 0.5484803253891997), (6, 0.31226270667960454), (8, 0.5484803253891997)]\n"
     ]
    }
   ],
   "source": [
    "# Create query document and convert to tf-idf\n",
    "# doc shares two words with each of first two docs in corpus\n",
    "query_doc = \"chicken with tacos love\".split()\n",
    "print(query_doc)\n",
    "query_doc_bow = dictionary.doc2bow(query_doc)\n",
    "print(query_doc_bow)\n",
    "query_doc_tf_idf = tf_idf[query_doc_bow]\n",
    "print(query_doc_tf_idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.71954989,  0.34925455,  0.        ,  0.06428327,  0.        ], dtype=float32)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show array of document similarities to query\n",
    "# Also both document 0 and 1 match with two words each,\n",
    "# document 1 matches with word \"with\" that occurs twice in corpus.\n",
    "# Only one overlapping word with the fourth document\n",
    "sims[query_doc_tf_idf]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Exercise: take a document like an email or news article and find sentences most similar to input query sentences."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
