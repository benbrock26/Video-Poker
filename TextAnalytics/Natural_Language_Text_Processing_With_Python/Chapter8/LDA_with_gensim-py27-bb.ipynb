{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA in Gensim\n",
    "https://radimrehurek.com/gensim/models/ldamodel.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# want to make clean words and return a list of tokens\n",
    "\n",
    "#from spacy.en import English\n",
    "#parser = English()\n",
    "\n",
    "import spacy\n",
    "parser = spacy.load('en')\n",
    "\n",
    "def tokenize(text):\n",
    "    lda_tokens = []\n",
    "    tokens = parser(text)\n",
    "    for token in tokens:\n",
    "        if token.orth_.isspace():\n",
    "            continue\n",
    "        elif token.like_url:\n",
    "            lda_tokens.append('URL')\n",
    "        elif token.orth_.startswith('@'):\n",
    "            lda_tokens.append('SCREEN_NAME')\n",
    "        else:\n",
    "            lda_tokens.append(token.lower_)\n",
    "    return lda_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A lot of times you will get text from some unknown encoding. \n",
    "# UTF-8 is the most common representation.\n",
    "# If you need to catch errros, you can remove errors='ignore'\n",
    "def convert_unicode(text):\n",
    "    if isinstance(text,str):\n",
    "        return text.decode('utf-8',errors='ignore')\n",
    "    else:\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['SCREEN_NAME', u'said', u'the', u'#', u'chicken', u'was', u'at', u'the', u'#', u'junkyard', u'.', u'see', 'URL', u'.']\n"
     ]
    }
   ],
   "source": [
    "sent = '@bob said the #chicken was at the #junkyard. See http://www.jonathanmugan.com.'\n",
    "out_tokens = tokenize(convert_unicode(sent))\n",
    "print(out_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# we want to lemmatize so dogs goes to dog and ran goes to run\n",
    "# Lemmatization means to get the \"dictionary entry\" for a word\n",
    "\n",
    "# Some documentation here http://www.nltk.org/howto/wordnet.html\n",
    "\n",
    "from nltk.corpus import wordnet as wn\n",
    "def get_lemma(word):\n",
    "    lemma = wn.morphy(word)\n",
    "    if lemma is None:\n",
    "        return word\n",
    "    else: \n",
    "        return lemma\n",
    "    \n",
    "# or can use this\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "def get_lemma2(word):\n",
    "    return WordNetLemmatizer().lemmatize(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('dogs', u'dog', u'dog')\n",
      "('ran', u'run', 'ran')\n",
      "('discouraged', u'discourage', 'discouraged')\n"
     ]
    }
   ],
   "source": [
    "for w in ['dogs','ran','discouraged']:\n",
    "    print(w,get_lemma(w),get_lemma2(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "en_stop = set(nltk.corpus.stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepare_text_for_lda(text):\n",
    "    tokens = tokenize(text)\n",
    "    tokens = [token for token in tokens if len(token) > 4]\n",
    "    tokens = [token for token in tokens if token not in en_stop]\n",
    "    tokens = [get_lemma(token) for token in tokens]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'enjoy', u'going', u'restaurant', u'hamburger']\n"
     ]
    }
   ],
   "source": [
    "sent = 'I enjoy going to restaurants to eat hamburgers.'\n",
    "print(prepare_text_for_lda(convert_unicode(sent)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'recently', u'finish', u'star', u'peter', u'heller', u'beautiful', u'especially', u'middle', u'enjoy', u'apocalypse']\n",
      "[u'remember', u'waiting', u'people', u'phone', u'would', u'sometimes', u'amaze']\n",
      "[u'still', u'diner', u'cards']\n",
      "[u'coach', u'announce', u'soccer', u'practice', u'optional', u'going', u'optional']\n",
      "[u'recently', u'watch', u'stripe', u'pajama']\n",
      "[u'buy', u'bottle', u'whole', u'food', u'try', u'confirm', u'appropriately', u'price']\n",
      "[u'recently', u'watch', u'pretty', u'movie', u'wilson', u'tiger']\n",
      "[u'drove', u'water', u'softener', u'front', u'hassle', u'carsarestupid']\n",
      "[u'attest', u'derisive', u'weekend', u'warrior', u'usually', u'recover', u'sunday', u'soccer', u'wednesday', u'oldman']\n",
      "[u'culture', u'tip', u'things', u'valet', u'parking', u'culture', u'carry', u'something']\n",
      "[u'spend', u'try', u'little', u'notification', u'icon', u'skinner']\n",
      "[u'expose', u'travel', u'always', u'engross', u'thing', u'important']\n",
      "[u'taste', u'think', u'taking', u'omega-3', u'thing', u'little']\n",
      "[u'excite', u'could', u'would', u'improve', u'improve']\n",
      "[u'javascript', u'become', u'little', u'robot', u'react', u'environment', u'browser', u'download', u'dynamical', u'system']\n",
      "[u'meditate', u'recognize', u'become']\n",
      "[u'finally', u'reading', u'count', u'monte', u'cristo', u'interest', u'inspire', u'favorite', u'movie', u'shawshank', u'redemption']\n",
      "[u'amaze', u'worthless', u'would', u'10,000', u'years', u'could', u'describe', u'amaze', u'technology', u'could', u'build']\n",
      "[u'believe', u'still', u'penny', u'worth', u'picking', u'ground']\n",
      "[u'thought', u'things', u'25-year', u'worker', u'asking', u'hear', u'name', u'magic']\n",
      "[u'instead', u'viewing', u'customer', u'service', u'company', u'treat', u'investment', u'information', u'gathering']\n",
      "[u'watch', u'sleeping', u'beauty', u'daughter', u'forget', u'laptop', u'going', u'office', u'sword', u'truth', u'shield', u'virtue']\n",
      "[u'install', u'software', u'spit', u'page', u'error', u'everything', u'sucessfully', u'instal']\n",
      "[u'eating', u'freeze', u'veggie', u'burger', u'lately', u'trick', u'hamburger', u'quick', u'protein']\n",
      "[u'table', u'hide', u'obstruction', u'underneath', u'knock', u'knee', u'add', u'benefit', u'embarrass']\n",
      "[u'wonder', u'groom', u'repeat']\n",
      "[u'things', u'could']\n",
      "[u'interest', u'contradiction', u'learning', u'things', u'make']\n",
      "[u'wonder', u'think', u'intelligence', u'ability', u'specific']\n",
      "[u'randy', u'pausch', u'raise', u'worth', u'police', u'never', u'cross', u'something']\n",
      "[u'texted', u'switching', u'parking', u'space', u'driveway', u'snake', u'found', u'marriage', u'texas']\n",
      "[u'nothing', u'excite', u'depress', u'figuring', u'cause', u'happy', u'regret', u'waste']\n",
      "[u'field', u'study', u'years', u'someone', u'always', u'safe', u'fields', u'begin']\n",
      "[u'dream', u'attack', u'merica', u'drop', u'large', u'style', u'television', u'house', u'helicopter', u'scary', u'sound']\n",
      "[u'billion', u'protein', u'working', u'every', u'human', u'complicate', u'machine']\n",
      "[u'endeavour', u'teach', u'child', u'probability', u'using', u'flip', u'first', u'seven', u'flip', u'tails', u'mental', u'model', u'never', u'recover']\n",
      "[u'working', u'daughter', u'robot', u'would', u'could', u'learn', u'programming', u'robot']\n",
      "[u'superman', u'space', u'alien', u'generally', u'think']\n",
      "[u'like', u'watch', u'snake', u'digging', u'bush', u'pulling', u'weeds', u'laugh', u'laugh']\n",
      "[u'clearing', u'house', u'build', u'subdivision', u'call', u'preserve']\n",
      "[u'still', u'remember', u'metallic', u'smell', u'incredible', u'lunch', u'first', u'grade']\n",
      "[u'reading', u'light', u'august', u'william', u'faulkner', u'little', u'tedious', u'bitchery', u'abomination', u'soccer']\n",
      "[u'going', u'advice', u'birthday', u'learn', u'advice']\n",
      "[u'spoon', u'little', u'nick', u'outside']\n",
      "[u'forget', u'return', u'redbox', u'movie', u'beginning', u'think', u'absent', u'mindedness', u'entire', u'business', u'model']\n",
      "[u'wonder', u'advance', u'alien', u'race', u'version', u'incompatibility', u'software', u'change', u'function', u'signature']\n",
      "[u'guess', u'fascination', u'apocalypse', u'come', u'desire', u'revert', u'hunter', u'gatherer', u'escape', u'nature']\n",
      "[u'balloon', u'fill', u'helium', u'floating', u'death', u'apparently']\n",
      "[u'today', u'howmydaughterplaysstarwars']\n",
      "[u'finish', u'reading', u'sleep', u'chandler', u'detective', u'story', u'arguably', u'start', u'singular', u'writing']\n",
      "[u'write', u'understandable', u'explanation', u'learning', u'natural', u'language', u'processing', u'deeplearning']\n",
      "[u'luxury', u'silence', u'truck', u'worst', u'follow', u'blower', u'television']\n",
      "[u'computer', u'scientist', u'trouble', u'source', u'person', u'simple']\n",
      "[u'python', u'programmer', u'keyword', u'confuse', u'superfluous']\n",
      "[u'robot', u'repair', u'ask', u'going', u'person']\n",
      "[u'package', u'microwave', u'promise', u'expand', u'culinary', u'horizon', u'freeze', u'dinner']\n",
      "[u'francis', u'seem', u'surprisingly', u'reasonable']\n",
      "[u'denver', u'never', u'eagle', u'plenty', u'turkey', u'vulture']\n",
      "[u'pointer', u'memory']\n",
      "[u'cite', u'david', u'attenborough', u'cyber', u'security', u'document']\n",
      "[u'protip', u'making', u'flight', u'baltimore', u'airport', u'turkey']\n",
      "[u'great', u'question', u'technical', u'interview', u'dream', u'technology', u'allow', u'values', u'scope', u'thought']\n",
      "[u'ask', u'build', u'great', u'china', u'mongolian', u'learn', u'southpark']\n",
      "[u'introduction', u'summary', u'material', u'reader', u'teaching', u'raising', u'question', u'amwriting']\n",
      "[u'understand', u'anyone', u'freebird', u'burrito', u'basically', u'wrap', u'tortilla', u'carbsallthewaydown']\n",
      "[u'italian', u'call', u'original', u'custom', u'anything', u'besides', u'italian', u'sound', u'pretentious', u'false']\n",
      "[u'dream', u'play', u'pickup', u'soccer', u'group', u'people', u'lose', u'would', u'try', u'hard', u'know']\n",
      "[u'nothing', u'truly', u'happen', u'everything', u'unnecessarily', u'difficult', u'yesterday', u'today']\n",
      "[u'wonder', u'companionship', u'important', u'although', u'youtube', u'would']\n",
      "[u'someday', u'people', u'natural', u'simplicity']\n",
      "[u'great', u'audiobook']\n",
      "[u'motorize', u'vehicle', u'ban', u'imagine', u'peace', u'quiet']\n",
      "[u'worry', u'enough', u'worry', u'relax', u'enough', u'course', u'worry', u'worry']\n",
      "[u'dream', u'playing', u'little', u'league', u'baseball', u'drove', u'garbage', u'truck']\n",
      "[u'sympathize', u'view', u'trouble', u'trust', u'michael', u'moore', u'since', u'blame', u'clark', u'death', u'little']\n",
      "[u'thing', u'learn', u'kind', u'soccer', u'coach', u'weather', u'make', u'shoes', u'untie', u'themoreyouknow']\n",
      "[u'message', u'jonathan', u'return', u'yellow', u'camper', u'highway', u'yellow', u'camper']\n",
      "[u'library', u'wallet', u'wonder', u'direct', u'librarian', u'twitter', u'valid', u'photo']\n",
      "[u'figure', u'walking', u'sound', u'footstep']\n",
      "[u'pleasant', u'surprise', u'morning', u'tighten']\n",
      "[u'rhythm', u'vowel']\n",
      "[u'watch', u'tyrannosaur', u'movie', u'break', u'heart', u'fewer', u'dinosaur', u'expect']\n",
      "[u'sometimes', u'writing', u'try', u'articulate', u'idea', u'everything', u'seem']\n",
      "[u'coffee', u'running', u'around', u'remind', u'lebowski', u'careful', u'beverage']\n",
      "[u'typewriter', u'today', u'blow', u'thing', u'alien', u'world']\n",
      "[u'enough', u'cramp', u'scream', u'soccer']\n",
      "[u'shame', u'coming', u'zombie', u'apocalypse', u'thing', u'still', u'bunker']\n",
      "[u'proposal', u'write', u'ask', u'write', u'boring', u'thought']\n",
      "[u'agree', u'everyone', u'know', u'correlation', u'equal', u'causation', u'life']\n",
      "[u'working', u'looking', u'forward', u'mow']\n",
      "[u'shock', u'going', u'digital', u'world', u'computer', u'science', u'analog', u'world', u'repair', u'nothing']\n",
      "[u'fasten', u'stuff', u'flying', u'everywhere', u'skinner', u'pigeon', u'learn', u'fast']\n",
      "[u'restaurant', u'taste', u'better', u'compare', u'item', u'compare', u'sandwich']\n",
      "[u'realize', u'liquid', u'coffee', u'spill']\n",
      "[u'motto', u'future', u'information', u'relevant']\n",
      "[u'printer', u'stubborn', u'trust', u'paper']\n",
      "[u'reading', u'american', u'character', u'amaze', u'dream', u'dream', u'follow', u'around', u'grocery', u'store', u'chain', u'smoking', u'gorilla']\n",
      "[u'write', u'later', u'remember', u'refer']\n",
      "[u'google', u'internet', u'internet']\n",
      "[u'roast', u'serving', u'container', u'vary', u'serving', u'exactly', u'calorie']\n",
      "[u'funny', u'matter', u'right', u'spend', u'couple', u'hours', u'piddle', u'around']\n",
      "[u'excuse', u'printer', u'black', u'white', u'people', u'color', u'expensive', u'poppycock', u'color']\n",
      "[u'hands', u'think', u'saying', u'make', u'sense']\n",
      "[u'reach', u'repeating', u'stage', u'cognitive', u'development', u'piaget', u'would']\n",
      "[u'shopping', u'tonight', u'first', u'years', u'amaze', u'mall', u'change', u'since', u'teenager']\n",
      "[u'bummer', u'getting', u'older', u'already', u'every', u'movie', u'come']\n",
      "[u'admit', u'people', u'refer', u'something', u'cynical', u'starting', u'doubt', u'either']\n",
      "[u'funny', u'sound', u'person', u'suppose', u'almost', u'someone', u'barely', u'audible']\n",
      "[u'place', u'landscape', u'style', u'apocalypse', u'roadway', u'sidewalk', u'clear', u'nature']\n",
      "[u'chin', u'somebody', u'need', u'something', u'parent']\n",
      "[u'glass', u'water', u'amaze', u'clean', u'water', u'grant', u'drink', u'water']\n",
      "[u'online', u'shopping', u'infinite', u'forget', u'field', u'error', u'saying', u'forget', u'field', u'randomly', u'clear', u'another', u'field', u'repeat']\n",
      "[u'change', u'nickel', u'think', u'everywhere', u'nickel']\n",
      "[u'member', u'secret', u'service', u'unload', u'parking', u'clear', u'toddler']\n",
      "[u'remember', u'first', u'email', u'angry', u'reply', u'stern', u'lecture', u'things', u'unacceptable']\n",
      "[u'video', u'game', u'design', u'exploit', u'psychological', u'desire', u'actually', u'achieve', u'something']\n",
      "[u'understand', u'society', u'blower', u'peace', u'quiet', u'tidiness']\n",
      "[u'need', u'statistics', u'science', u'program', u'parameter', u'report']\n",
      "[u'wrestling', u'child', u'make', u'caveman', u'primal']\n",
      "[u'inception', u'eternal', u'sunshine', u'spotless', u'dream', u'similar']\n",
      "[u'mathematician', u'drive', u'publish', u'whole', u'something', u'category', u'theory', u'never']\n",
      "Wall time: 27 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# get the data\n",
    "import random\n",
    "text_data = []\n",
    "with open('jonathan_mugan_tweets.txt') as f:\n",
    "    for line in f:\n",
    "        tokens = prepare_text_for_lda(convert_unicode(line))\n",
    "        if random.random() > .95:\n",
    "            print(tokens)\n",
    "        text_data.append(tokens)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ben Brock\\Anaconda3\\envs\\py27_gensim_spacy\\lib\\site-packages\\gensim\\utils.py:860: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "# create a dictionary from the data\n",
    "from gensim import corpora\n",
    "dictionary = corpora.Dictionary(text_data)\n",
    "\n",
    "# Warning message shows that you can also do lemmatization through Pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# convert to bag-of-words corpus\n",
    "corpus = [dictionary.doc2bow(text) for text in text_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 171 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# save the corpus and dictionary, we will use these in another video to visualize\n",
    "import pickle\n",
    "pickle.dump( corpus, open( \"corpus-py27.pkl\", \"wb\" ) )\n",
    "dictionary.save('dictionary-py27.gensim')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 25.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import gensim\n",
    "NUM_TOPICS = 5\n",
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=NUM_TOPICS,\n",
    "                                           id2word= dictionary, passes = 15)\n",
    "ldamodel.save('model5-py27.gensim')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, u'0.010*\"watch\" + 0.008*\"think\" + 0.008*\"going\" + 0.008*\"wonder\"')\n",
      "(1, u'0.014*\"people\" + 0.010*\"coffee\" + 0.009*\"would\" + 0.007*\"picture\"')\n",
      "(2, u'0.010*\"people\" + 0.009*\"change\" + 0.008*\"write\" + 0.007*\"funny\"')\n",
      "(3, u'0.012*\"funny\" + 0.012*\"dream\" + 0.007*\"child\" + 0.006*\"anymore\"')\n",
      "(4, u'0.012*\"would\" + 0.011*\"could\" + 0.011*\"remember\" + 0.009*\"never\"')\n",
      "Wall time: 16 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "topics = ldamodel.print_topics(num_words=4)\n",
    "for topic in topics:\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(3, 1), (190, 1)]\n",
      "[(0, 0.73031123020059985), (1, 0.067371280336751493), (2, 0.067096699075636451), (3, 0.067087415721834839), (4, 0.068133374665177465)]\n",
      "Wall time: 88 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# try a new document\n",
    "# we see it is mostly topic 3\n",
    "new_doc = 'I watch movies.'\n",
    "new_doc = prepare_text_for_lda(convert_unicode(new_doc))\n",
    "new_doc_bow = dictionary.doc2bow(new_doc)\n",
    "print(new_doc_bow)\n",
    "print(ldamodel.get_document_topics(new_doc_bow))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, u'0.010*\"would\" + 0.010*\"remember\" + 0.009*\"could\" + 0.007*\"funny\"')\n",
      "(1, u'0.013*\"movie\" + 0.012*\"watch\" + 0.011*\"people\" + 0.010*\"would\"')\n",
      "(2, u'0.010*\"dream\" + 0.007*\"people\" + 0.007*\"reading\" + 0.006*\"amaze\"')\n",
      "Wall time: 27.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# try three topics\n",
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=3,\n",
    "                                           id2word= dictionary, passes = 15)\n",
    "ldamodel.save('model3-py27.gensim')\n",
    "topics = ldamodel.print_topics(num_words=4)\n",
    "for topic in topics:\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, u'0.014*\"child\" + 0.013*\"sugar\" + 0.010*\"movie\" + 0.009*\"perfect\"')\n",
      "(1, u'0.014*\"could\" + 0.011*\"google\" + 0.009*\"getting\" + 0.009*\"people\"')\n",
      "(2, u'0.015*\"enough\" + 0.014*\"funny\" + 0.013*\"really\" + 0.013*\"watch\"')\n",
      "(3, u'0.018*\"could\" + 0.016*\"place\" + 0.013*\"first\" + 0.011*\"things\"')\n",
      "(4, u'0.021*\"funny\" + 0.015*\"something\" + 0.014*\"always\" + 0.014*\"seem\"')\n",
      "(5, u'0.030*\"remember\" + 0.019*\"want\" + 0.017*\"memory\" + 0.014*\"friend\"')\n",
      "(6, u'0.013*\"people\" + 0.012*\"think\" + 0.011*\"commercial\" + 0.010*\"change\"')\n",
      "(7, u'0.022*\"would\" + 0.016*\"people\" + 0.014*\"coffee\" + 0.014*\"wonder\"')\n",
      "(8, u'0.020*\"robot\" + 0.014*\"machine\" + 0.013*\"every\" + 0.012*\"person\"')\n",
      "(9, u'0.016*\"dream\" + 0.015*\"thing\" + 0.015*\"around\" + 0.011*\"would\"')\n",
      "Wall time: 26.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# try ten topics\n",
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=10,\n",
    "                                           id2word= dictionary, passes = 15)\n",
    "ldamodel.save('model10-py27.gensim')\n",
    "topics = ldamodel.print_topics(num_words=4)\n",
    "for topic in topics:\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No handlers could be found for logger \"sklearn.datasets.twenty_newsgroups\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['DESCR', 'data', 'description', 'filenames', 'target', 'target_names']\n",
      "11314\n",
      "[7 4 4 ..., 3 1 8]\n",
      "['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n",
      "From: lerxst@wam.umd.edu (where's my thing)\n",
      "Subject: WHAT car is this!?\n",
      "Nntp-Posting-Host: rac3.wam.umd.edu\n",
      "Organization: University of Maryland, College Park\n",
      "Lines: 15\n",
      "\n",
      " I was wondering if anyone out there could enlighten me on this car I saw\n",
      "the other day. It was a 2-door sports car, looked to be from the late 60s/\n",
      "early 70s. It was called a Bricklin. The doors were really small. In addition,\n",
      "the front bumper was separate from the rest of the body. This is \n",
      "all I know. If anyone can tellme a model name, engine specs, years\n",
      "of production, where this car is made, history, or whatever info you\n",
      "have on this funky looking car, please e-mail.\n",
      "\n",
      "Thanks,\n",
      "- IL\n",
      "   ---- brought to you by your neighborhood Lerxst ----\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Wall time: 1min 17s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Exercise: Run LDA on Newsgroup Data\n",
    "# The Newsgroup Data\n",
    "# http://scikit-learn.org/stable/datasets/twenty_newsgroups.html#newsgroups\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "texts = fetch_20newsgroups(subset='train')\n",
    "print(dir(texts))\n",
    "# 11,314 posts\n",
    "print(len(texts.target))\n",
    "print(texts.target)\n",
    "print(texts.target_names)\n",
    "print(texts.data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
